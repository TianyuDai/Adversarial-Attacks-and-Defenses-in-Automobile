{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Attacks and Defenses in Automobile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "possible references: \n",
    "- [Practical Black-Box Attacks against Machine Learning](https://arxiv.org/pdf/1602.02697.pdf)\n",
    "- [Adversarial Attacks and Defences: A Survey](https://arxiv.org/abs/1810.00069)\n",
    "- [BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain](https://arxiv.org/abs/1708.06733)\n",
    "- [Deep learning for large scale traffic sign detection and recognition](https://arxiv.org/pdf/1904.00649v1.pdf)\n",
    "- [Adversarial Attacks on Neural Network Policies](https://arxiv.org/abs/1702.02284)\n",
    "- [Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks](https://arxiv.org/abs/1701.04143)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Goal\n",
    "Explore adversarial attack approaches for traffic sign recognition, and ways to defense against the attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Outline\n",
    "- Literature review: Oct. 25th - 31st\n",
    "- Familiarize yourself with US traffic sign dataset: Oct. 31st - Nov. 7th\n",
    "- Review ways for non target attack and target attack\n",
    "- Compare ways for defenses against the attacks: Nov. 7th - Nov. 14th\n",
    "- Propose your own defense system against the attacks: Nov. 14th - Nov. 28th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US traffic sign dataset\n",
    "- [LISA traffic sign dataset and implementation](https://git-disl.github.io/GTDLBench/datasets/lisa_traffic_sign_dataset/)\n",
    "- [Mapillary Traffic Sign Dataset](https://blog.mapillary.com/update/2019/06/27/mapillary-traffic-sign-dataset.html)\n",
    "- [DFG traffic sign dataset](https://paperswithcode.com/sota/traffic-sign-recognition-on-dfg-traffic-sign-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US traffic sign detection approaches\n",
    "[Traffic Sign Detection for U.S. Roads: Remaining Challenges and a case for Tracking](https://ieeexplore.ieee.org/document/6957882)\n",
    "- LISA Traffic Sign Dataset\n",
    "- Integral Channel Features work well for colorful, distinctively shaped signs, but fail on large superclass of speed limit signs and similar designs.\n",
    "- In order to be useful for higher level analysis, any traffic sign detection system should contain tracking.\n",
    "\n",
    "[Deep Learning for Large-Scale Traffic-Sign Detection and Recognition](https://ieeexplore.ieee.org/document/8709983)\n",
    "- DFG traffic sign dataset (with benchmark)\n",
    "- Use Mask R-CNN with adaptations for traffic signs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test AIâ€™s vulnerabilities to adversarial examples\n",
    "[cleverhans library](https://github.com/cleverhans-lab/cleverhans): An open-source implementation of adversarial training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial defense\n",
    "[openAI introductory article](https://openai.com/blog/adversarial-example-research/)\n",
    "\n",
    "Weight decay and dropout do not provide a practical defense against adversarial examples\n",
    "\n",
    "So far, only two methods have provided a significant defense: \n",
    "- Adversarial training: This is a brute force solution where we simply generate a lot of adversarial examples and explicitly train the model not to be fooled by each of them. An open-source implementation of adversarial training is available in the cleverhans library and its use illustrated in the following tutorial.\n",
    "- Defensive distillation: This is a strategy where we train the model to output probabilities of different classes, rather than hard decisions about which class to output. The probabilities are supplied by an earlier model, trained on the same task using hard class labels. This creates a model whose surface is smoothed in the directions an adversary will typically try to exploit, making it difficult for them to discover adversarial input tweaks that lead to incorrect categorization. (Distillation was originally introduced in Distilling the Knowledge in a Neural Network as a technique for model compression, where a small model is trained to imitate a large one, in order to obtain computational savings.)\n",
    "\n",
    "Yet even these specialized algorithms can easily be broken by giving more computational firepower to the attacker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
